---
title: "R Notebook"
output: html_notebook
---

```{r, include=FALSE}
library(AppliedPredictiveModeling)
library(Amelia)
library(doParallel)
library(missForest)
library(cwhmisc)
library(MASS)
library(forecast)
library(tidyverse)
library(pls)
library(caret)
```

# Chapter 6 (HW 6.3)

A chemical manufacturing process for a pharmaceutical product was discussed in Sect. 1.4. In this problem, the objective is the understand the relationship between biological measurements of the raw materials (predictors), measurements of the manufacturing process (predictors), and the response of product yield. Biological predictors cannot be changed but can be used to assess the quality of the raw material before processing. On the other hand, manufacturing process predictors can be changed in the manufacturing process. Improving product yield by 1 % will boost revenue by approximately one hundred thousand dollars per batch. 

a. Start $ and use the commands to load the data:

```{r}
library(AppliedPredictiveModeling)
data("ChemicalManufacturingProcess")
```

The matrix processPredictors contains the 57 predictors (12 describing the input biological material and 45 describing the process predictors) for the 176 manufacturing runs. yield contains the percent yield for each run.

b. A small percentage of cells in the predictor set contains missing values. Use an imputation function to fill in these missing values.

```{r, cache=TRUE}
library(Amelia)
missmap(ChemicalManufacturingProcess)
```
> It appears that the missing values are quite minor and seem to be missing at random. 

```{r, cache=TRUE}
library(doParallel)
library(missForest)

registerDoParallel(cores=3)
chem.imput <- missForest(ChemicalManufacturingProcess, ntree = 100, parallelize = "forests")$ximp
data.set <- list(org.data.set = chem.imput)
```

c. Split the data into a training and a test set, pre-process the data, and tune a model of your choice from this chapter. What is the optimal value of the performance metric?

> Lets explore the distribution of the variables

```{r, cache=TRUE}
library(psych)
multi.hist(data.set$org.data.set[1:8])
multi.hist(data.set$org.data.set[9:16])
multi.hist(data.set$org.data.set[17:24])
multi.hist(data.set$org.data.set[25:32])
```

The distributions of several of the variables are noticeably skewed, so we will obtain the box cox transformation lambda

```{r, cache=TRUE}
variables.to.transform <- c("BiologicalMaterial04", "ManufacturingProcess01",
                            "ManufacturingProcess02","ManufacturingProcess03",
                            "ManufacturingProcess06", "ManufacturingProcess07",
                            "ManufacturingProcess08","ManufacturingProcess12",
                            "ManufacturingProcess16","ManufacturingProcess18") 

results <- NULL
for (i in 1:length(variables.to.transform)){
  results[[i]] <- list(variable = variables.to.transform[[i]],
                       lambda = BoxCox.lambda(data.set$org.data.set[,variables.to.transform[[i]]]))}
results
```

> We have our lambda transformations and we can apply them to our data set then set our test and train splices. 

```{r, cache=TRUE}
trans.data.set <- data.set$org.data.set %>% 
                    mutate(BiologicalMaterial04.boxcoxtrans = 
                             BoxCox(BiologicalMaterial04, -0.99992424816297),
                           ManufacturingProcess01.boxcoxtrans = 
                             BoxCox(ManufacturingProcess01, 1.99995900720725),
                           ManufacturingProcess02.boxcoxtrans = 
                             BoxCox(ManufacturingProcess02, 1.99995900720725),
                           ManufacturingProcess03.boxcoxtrans = 
                             BoxCox(ManufacturingProcess03, -0.99992424816297),
                           ManufacturingProcess06.boxcoxtrans = 
                             BoxCox(ManufacturingProcess06, -0.99992424816297),
                           ManufacturingProcess07.boxcoxtrans = 
                             BoxCox(ManufacturingProcess07,-0.99992424816297),
                           ManufacturingProcess08.boxcoxtrans = 
                             BoxCox(ManufacturingProcess08, 1.99992424816297),
                           ManufacturingProcess12.boxcoxtrans = 
                             BoxCox(ManufacturingProcess12, 0.0000410225926326142),
                           ManufacturingProcess16.boxcoxtrans = 
                             BoxCox(ManufacturingProcess16, 1.99995900720725),
                           ManufacturingProcess18.boxcoxtrans = 
                             BoxCox(ManufacturingProcess18, 1.99995900720725))
```

> Splitting the test and training data set 

```{r}
set.seed(123)
```

```{r, cache=TRUE}
data.set <- c(data.set, list(trans.data.set = trans.data.set))  %>% 
            c(., list(train.test.split = floor(0.75 * nrow(chem.imput)))) %>% 
            c(., train.index = list(sample(seq_len(nrow(chem.imput)), size = .$train.test.split))) %>%
            c(., train = list(as.data.frame(trans.data.set[.$train.index,])), test = list(as.data.frame(trans.data.set[-.$train.index,])))
```

> Now we can proceed with the model of choice which is the Partial Least Squares. Note that the evaluation below relied heavily on this rpubs post- https://rpubs.com/omicsdata/pls.

```{r}
library(pls)
chem.plsFit <- plsr(Yield ~ ., data =  data.set$train, validation = "CV")
validationplot(chem.plsFit, val.type="RMSEP")
```

> We see a steep increase at 10 components followed by many mountains and valleys. Our intention is to select the most optimal number of components to minimize the RMSE. 

```{r}
pls.RMSEP <- RMSEP(chem.plsFit, estimate="CV") 
plot(pls.RMSEP, main="RMSEP PLS Solubility", xlab="components")
min_comp <- which.min(pls.RMSEP$val)
points(min_comp, min(pls.RMSEP$val), pch=1, col="red", cex=1.5)
```

> Our optimal number of components is 4. 

[1] RPubs - Partial Least Square Regression. N.p., n.d. Web. 14 Nov. 2017. - https://rpubs.com/omicsdata/pls


d. Predict the response for the test set. What is the value of the performance metric and how does this compare with the resampled performance metric on the training set?

> Our results from our training set 

```{r}
chem.plsPredict.train <-  predict(chem.plsFit, data = data.set$train, ncomp = min_comp)
pls.eval <- data.frame(obs = data.set$train[,1], pred = chem.plsPredict.train[,1,1])
defaultSummary(pls.eval)
```

> Results from the test set

```{r}
chem.plsPredict <- predict(chem.plsFit, data = data.set$test, ncomp = min_comp)
pls.eval <- data.frame(obs = data.set$test[,1], pred = chem.plsPredict[,1,1])
defaultSummary(pls.eval)
```

> While our RMSE is within range of both the training and test data set the Rsquared is significantly less for the test set suggesting a model that does not explain much of the variation. This may not be the most optimal model. 

e. Which predictors are most important in the model you have trained? Do either the biological or process predictors dominate the list?

```{r}
library(data.table)
dt <- setDT(varImp(chem.plsFit), keep.rownames = TRUE)[]
options(scipen=99)
dt[order(-Overall)]
```

> We see that the Manufacturing Processes dominate the list for variable importance with a few biological materials having importance.  

f. Explore the relationships between each of the top predictors and the response. How would this information be helpful in improving yield in future runs of the manufacturing process?

> The top manufacturing process is ManufacturingProcess40

```{r}
data.set$org.data.set %>% select(Yield, ManufacturingProcess40) %>% cor() %>% .[2]
```

We see a negative correlation ManufacturingProcess40 and Yield, which suggests that ManufacturingProcess40 represents a process that can decrease the Yield. We cannot say that ManufacturingProcess40 itself decrease Yield, only that a negative relationship exists. 

> The top biological process is BiologicalMaterial12

```{r}
data.set$org.data.set %>% select(Yield, BiologicalMaterial12) %>% cor() %>% .[2]
```

We actually see the opposite for the Biological indicator in that there is a positive correlation to Yield. We cannot say that BiologicalMaterial12 itself improves Yield, only that a positive relationship exists. 
