---
title: "Final Project"
author: "Christophe Hunt"
date: "May 13, 2017"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    includes:
      in_header: header.tex
    number_sections: yes
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: '2'
always_allow_html: yes
---

```{r load libraries, include=FALSE}
library(tidyverse)
library(scales)
library(forecast)
library(knitr)
library(stargazer)
```


Pick one of the quantitative independent variables from the training data set (train.csv), and define that variable as X.

Pick SalePrice as the dependent variable, and define it as Y for the next analysis.   

```{r read train data, message = FALSE, message=FALSE, tidy=TRUE, cache=TRUE}
library(tidyverse)
train.df  <- as_tibble(read.csv("https://raw.githubusercontent.com/ChristopheHunt/MSDA---Coursework/master/Data%20605/Final%20Project/train.csv"))
```

```{r subset train data,  message = FALSE, message=FALSE, tidy=TRUE, cache=TRUE}
sub.train.df <- train.df[,c("SalePrice", "LotArea")]
```

> The variable we will set to X is LotArea, which is defined as the Lot size in square feet. I chose this because an anecdotal assumption is that the larger the lot size is the higher the sale price. However, living in NYC there are tiny lots in very desirable places that have a high price so I believe there may be some interesting patterns here. 

# Probability   

Calculate as a minimum the below probabilities a through c.  

Assume the small letter "x" is estimated as the 4th quartile of the X variable, and the small letter "y" is estimated as the 2nd quartile of the Y variable.  Interpret the meaning of all probabilities.  

  a. $P(X>x | Y>y)$	    
  
```{r,message = FALSE, message=FALSE, cache=TRUE}
prob.x <- list(qrtx = as.numeric(quantile(sub.train.df$LotArea)[4]), 
               mean = mean(sub.train.df$LotArea), 
               std = sd(sub.train.df$LotArea))

prob.y <- list(qrtx = as.numeric(quantile(sub.train.df$SalePrice)[2]), 
               mean = mean(sub.train.df$SalePrice), 
               std = sd(sub.train.df$SalePrice))
```

  
  b. $P(X>x, Y>y)$	
  
  c. $P(X<x | Y>y)$

Does splitting the training data in this fashion make them independent? 

In other words, does $P(X|Y)=P(X)P(Y))$?   

Check mathematically, and then evaluate by running a Chi Square test for association.  

You might have to research this.  


# Descriptive and Inferential Statistics. 

Provide univariate descriptive statistics and appropriate plots for both variables.   

Provide a scatterplot of X and Y.

```{r scatter plot,  message = FALSE, message=FALSE, tidy=TRUE, cache=TRUE, fig.height=4, fig.width=6}
ggplot(sub.train.df, aes(x = LotArea, y = SalePrice)) +
    geom_point(shape=1) +
    theme_light() +
    scale_y_continuous(labels = dollar)
```

Transform both variables simultaneously using Box-Cox transformations.  

> I am using the `BoxCox.lambda` function from the `forecast` package to determine the necessary transformations for the two variables.

```{r box cox table, cache=TRUE, eval=TRUE}
library(forecast)
library(knitr)
l1 <- BoxCox.lambda(as.numeric(sub.train.df$SalePrice))
l2 <- BoxCox.lambda(as.numeric(sub.train.df$LotArea))

lamdas <- c(l1, l2)
Variables <- c("SalePrice", "LotArea")
dfBoxCox <- as.data.frame(cbind(round(as.numeric(lamdas),4), Variables))
colnames(dfBoxCox) <- c("$\\lambda$", "Variables")
kable(dfBoxCox, align = c("c", "c"))
```

\centering

Common Box-Cox Transformations[^1] [^2]

\setlength{\tabcolsep}{12pt}

\begin{tabular}{ c c }
\hline
$\lambda$ & Y' \\ \hline
-0.5 &	$Y^{-0.5}~=~\frac{1}{\sqrt{(Y)}}$ \\
0	& $\log(Y)$ \\
.25  & $\sqrt[4]{Y}$
\end{tabular}

\justifying

Lambda values were truncated to the nearest tenth that match a common transformation as per the below table.

\centering

\begin{tabular}{ c c }
\hline
variable & variable transformation \\ \hline
SalePrice & $SalePrice^{-0.5}$ \\
LotArea & $log(LotArea)$ 
\end{tabular}

\justifying

\setlength{\tabcolsep}{6pt}

[^1]: Osborne, Jason W. "Improving your data transformations: Applying the Box-Cox transformation." Practical Assessment, Research & Evaluation 15.12 (2010): 1-9.

[^2]: [By Understanding Both the Concept of Transformation and the Box-Cox Method, Practitioners Will Be Better Prepared to Work with Non-normal Data.](https://www.isixsigma.com/tools-templates/normality/making-data-normal-using-box-cox-power-transformation/) . "Making Data Normal Using Box-Cox Power Transformation." ISixSigma. N.p., n.d. Web. 29 Oct. 2016.

##TODO Expand on the correlation analysis

Using the transformed variables, run a correlation analysis and interpret.

```{r, cache=TRUE, eval=TRUE}
sub.train.df.trans <- sub.train.df %>% 
                      mutate(SalePrice = SalePrice^(-.5), 
                             LotArea = log(LotArea))

sub.train.cor <- cor.test(sub.train.df.trans$SalePrice, 
                          sub.train.df.trans$LotArea, 
                          method = "pearson", conf.level = .99)
sub.train.cor
```

> The p-value of the correlation test is 2.2e-16 which is less than the significance level of alpha at .05. We are using the standard alpha as there is no indication another any other value for alpha should be used. We can therefore say that the log of lot size and sale price raised to the -.5 power are significantly correlated with a negative correlation coefficient of -0.386. 

Test the hypothesis that the correlation between these variables is 0 and provide a 99% confidence interval.  

> The correlation test has specifically done that for us and we can safely reject the null hypothesis as we see that our 99% confidence interval exists at the values (-0.441, -0.327) with a p-value < 2.2e-16.

Discuss the meaning of your analysis.

> This means two possible things could have occured, there is no correlation and this data set is pulled from an unusual set of house sales. Or, more likely with the values obtained, our assumption of 0 correlation is incorect and we have obtained a very typical data set and must reject the null hypothesis because correlation does exist. 

# Linear Algebra and Correlation.  

```{r correlation matrix}
A <- cor(sub.train.df.trans)
A
```
Invert your correlation matrix.(This is known as the precision matrix and contains variance inflation factors on the diagonal.)

```{r precision matrix}
B <- solve(A)
B
```

 Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix.
 
```{r}
corr.by.pre.M <- A %*% B
corr.by.pre.M
pre.by.corr.M <- B %*% A
pre.by.corr.M
```

# Calculus-Based Probability & Statistics

Many times, it makes sense to fit a closed form distribution to data. For your non-transformed independent variable, location shift it so that the minimum value is above zero.

```{r}
min(sub.train.df$LotArea)
```

> For the independent variable chosen, there are no zero values observed. This makes sense as we would expect the lot area to have some value and I would expect it to never be unobserved (at least estimates would be used). 

Then load the MASS package and run fitdistr to fit a density function of your choice. (See  https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/fitdistr.html). 

Find the optimal value of the parameters for this distribution, and then take 1000 samples from this distribution (e.g., rexp(1000) for an exponential).

Plot a histogram and compare it with a histogram of your non-transformed original variable.   

# Modeling
Build some type of regression model and submit your model to the competition board.  

Provide your complete model summary and results with analysis. 

Report your Kaggle.com user name and score.

Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix.