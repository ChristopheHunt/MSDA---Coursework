---
title: "Final Project"
author: "Christophe Hunt"
date: "May 13, 2017"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    includes:
      in_header: header.tex
    number_sections: yes
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: '2'
always_allow_html: yes
---

```{r load libraries, include=FALSE}
library(tidyverse)
library(scales)
library(forecast)
library(knitr)
library(stargazer)
library(MASS)
library(fitdistrplus)
library(caret)
library(Hmisc)
library(doParallel)
```

# Variable

Pick one of the quantitative independent variables from the training data set (train.csv), and define that variable as X.

Pick SalePrice as the dependent variable, and define it as Y for the next analysis.   

## Variable Picked 

> The variable we will set to X is LotArea, which is defined as the Lot size in square feet. I chose LotArea because an anecdotal assumption is that the larger the lot size is the higher the sale price. However, living in NYC, I know that tiny lots in very desirable places have sold for a high price so I believe there may be some interesting varability. 

```{r read train data, message = FALSE, message=FALSE, tidy=TRUE, cache=TRUE}
library(tidyverse)
train.df  <- as_tibble(read.csv(paste("https://raw.githubusercontent.com/", "ChristopheHunt/",
                                      "MSDA---Coursework/master", 
                                      "/Data%20605/Final%20Project/train.csv", sep = "")))
```

```{r subset train data,  message = FALSE, message=FALSE, tidy=TRUE, cache=TRUE}
sub.train.df <- train.df[,c("SalePrice", "LotArea")]
```

# Probability   

Calculate as a minimum the below probabilities a through c.  

Assume the small letter "x" is estimated as the 4th quartile of the X variable, and the small letter "y" is estimated as the 2nd quartile of the Y variable.  Interpret the meaning of all probabilities.  

```{r,message = FALSE, message=FALSE, cache=TRUE}
prob.x <- list(qrt = as.numeric(quantile(sub.train.df$LotArea)[4]))

prob.y <- list(qrt = as.numeric(quantile(sub.train.df$SalePrice)[2]))
```

```{r}
prob.y.x <- sub.train.df %>% mutate(greaterLotArea = ifelse(LotArea >= prob.x$qrt, 1, 0), 
                                     lesserLotArea = ifelse(LotArea < prob.x$qrt, 1, 0), 
                                     greaterSalePrice = ifelse(SalePrice >= prob.y$qrt,1, 0), 
                                     lesserSalePrice = ifelse(SalePrice < prob.y$qrt,1, 0))
```

##  a. $P(X>x | Y>y)$	    

```{r}
a <- (sum(ifelse(prob.y.x$greaterLotArea == 1 & 
                   prob.y.x$greaterSalePrice == 1, 1, 0)) 
      / nrow(prob.y.x)) / ((sum(prob.y.x$greaterLotArea) / nrow(prob.y.x)))
a
```

##  b. $P(X>x, Y>y)$	
  
```{r}
b <- sum(ifelse(prob.y.x$greaterLotArea == 1 & prob.y.x$greaterSalePrice == 1, 1, 0))/nrow(prob.y.x)
b
```

##  c. $P(X<x | Y>y)$
  
```{r}
c <- (sum(ifelse(prob.y.x$lesserLotArea == 1 & prob.y.x$greaterSalePrice == 1, 1, 0)) / nrow(prob.y.x)) / ((sum(prob.y.x$lesserLotArea) / nrow(prob.y.x)))
c
```

Does splitting the training data in this fashion make them independent? 

In other words, does $P(X|Y)=P(X)P(Y)$?   

> I am understanding this to mean does the probability of X>x given Y>y, which was answered for in part a. above, equal the probability of X>x mutiplied by Y>y

## Checked Mathematically
```{r}
X <- sum(prob.y.x$greaterLotArea)/ nrow(prob.y.x)
Y <- sum(prob.y.x$greaterSalePrice) / nrow(prob.y.x)
X * Y
```

```{r}
a == (X * Y)
```

## Chi Square test for association.  

```{r}
prob.table <- as.data.frame(rbind(cbind(sum(prob.y.x$lesserLotArea), sum(prob.y.x$greaterLotArea)), cbind(sum(prob.y.x$lesserSalePrice), sum(prob.y.x$greaterSalePrice))))
chisq.test(prob.table)
```

> We see that the p-value is quite low, lower than the assumptive .05, so we therefore reject the null hypothesis that the values are independent of each other. 

# Descriptive and Inferential Statistics. 

Provide univariate descriptive statistics and appropriate plots for both variables.   

Provide a scatterplot of X and Y.

```{r scatter plot,  message = FALSE, message=FALSE, tidy=TRUE, cache=TRUE, fig.height=4, fig.width=6}
ggplot(sub.train.df, aes(x = LotArea, y = SalePrice)) +
    geom_point(shape=1) +
    theme_light() +
    scale_y_continuous(labels = dollar)
```

Transform both variables simultaneously using Box-Cox transformations.  

> I am using the `BoxCox.lambda` function from the `forecast` package to determine the necessary transformations for the two variables.

```{r box cox table, cache=TRUE, eval=TRUE}
library(forecast)
library(knitr)
l1 <- BoxCox.lambda(as.numeric(sub.train.df$SalePrice))
l2 <- BoxCox.lambda(as.numeric(sub.train.df$LotArea))

lamdas <- c(l1, l2)
Variables <- c("SalePrice", "LotArea")
dfBoxCox <- as.data.frame(cbind(round(as.numeric(lamdas),4), Variables))
colnames(dfBoxCox) <- c("$\\lambda$", "Variables")
kable(dfBoxCox, align = c("c", "c"))
```

\centering

Common Box-Cox Transformations[^1] [^2]

\setlength{\tabcolsep}{12pt}

\begin{tabular}{ c c }
\hline
$\lambda$ & Y' \\ \hline
-0.5 &	$Y^{-0.5}~=~\frac{1}{\sqrt{(Y)}}$ \\
0	& $\log(Y)$ \\
.25  & $\sqrt[4]{Y}$
\end{tabular}

\justifying

Lambda values were truncated to the nearest tenth that match a common transformation as per the below table.

\centering

\begin{tabular}{ c c }
\hline
variable & variable transformation \\ \hline
SalePrice & $SalePrice^{-0.5}$ \\
LotArea & $log(LotArea)$ 
\end{tabular}

\justifying

\setlength{\tabcolsep}{6pt}

[^1]: Osborne, Jason W. "Improving your data transformations: Applying the Box-Cox transformation." Practical Assessment, Research & Evaluation 15.12 (2010): 1-9.

[^2]: [By Understanding Both the Concept of Transformation and the Box-Cox Method, Practitioners Will Be Better Prepared to Work with Non-normal Data.](https://www.isixsigma.com/tools-templates/normality/making-data-normal-using-box-cox-power-transformation/) . "Making Data Normal Using Box-Cox Power Transformation." ISixSigma. N.p., n.d. Web. 29 Oct. 2016.

##TODO Expand on the correlation analysis

Using the transformed variables, run a correlation analysis and interpret.

```{r, cache=TRUE, eval=TRUE, results='markup'}
sub.train.df.trans <- sub.train.df %>% 
                      mutate(SalePrice = SalePrice^(-.5), 
                             LotArea = log(LotArea))

sub.train.cor <- cor.test(sub.train.df.trans$SalePrice, 
                          sub.train.df.trans$LotArea, 
                          method = "pearson", conf.level = .99)
sub.train.cor
```

> The p-value of the correlation test is 2.2e-16 which is less than the significance level of alpha at .05. We are using the standard alpha as there is no indication another any other value for alpha should be used. We can therefore say that the log of lot size and sale price raised to the -.5 power are significantly correlated with a negative correlation coefficient of -0.386. 

Test the hypothesis that the correlation between these variables is 0 and provide a 99% confidence interval.  

> The correlation test has specifically done that for us and we can safely reject the null hypothesis as we see that our 99% confidence interval exists at the values (-0.441, -0.327) with a p-value < 2.2e-16.

Discuss the meaning of your analysis.

> This means two possible things could have occured, there is no correlation and this data set is pulled from an unusual set of house sales. Or, more likely with the values obtained, our assumption of 0 correlation is incorect and we have obtained a very typical data set and must reject the null hypothesis because correlation does exist. 

# Linear Algebra and Correlation.  

```{r correlation matrix, cache=TRUE, eval=TRUE, results='asis'}
A <- cor(sub.train.df.trans)
kable(A)
```

Invert your correlation matrix.(This is known as the precision matrix and contains variance inflation factors on the diagonal.)

```{r precision matrix, cache=TRUE, eval=TRUE, results='asis'}
B <- solve(A)
kable(B)
```

Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix.
 
```{r matrix multiplication, cache=TRUE, eval=TRUE, results='asis'}
corr.by.pre.M <- A %*% B
kable(corr.by.pre.M)
pre.by.corr.M <- B %*% A
kable(pre.by.corr.M)
```

# Calculus-Based Probability & Statistics

Many times, it makes sense to fit a closed form distribution to data. For your non-transformed independent variable, location shift it so that the minimum value is above zero.

```{r, cache=TRUE, eval=TRUE, results='asis'}
min(sub.train.df$LotArea)
```

> For the independent variable chosen, there are no zero values observed. This makes sense as we would expect the lot area to have some value and I would expect it to never be unobserved (at least estimates would be used). 

> However, if a shift was required something like the below could be used. 

```{r, cache=TRUE, eval=TRUE, results='asis'}
shift <- sub.train.df$LotArea + 1 
```

Then load the MASS package and run fitdistr to fit a density function of your choice. (See  https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/fitdistr.html).   

> First lets look at what distrubtion would best fit our data.     

```{r, cache=TRUE, eval=TRUE, results='asis'}
library(fitdistrplus)
descdist(sub.train.df$LotArea)
```

> There were too many issues in attempting to fit the beta distribution so the next best theoretical distribution was used - lognormal. 

```{r, cache=TRUE, eval=TRUE, results='asis'}
library(MASS)
fit.log <- fitdistr(sub.train.df$LotArea, densfun = "log-normal")
```


Find the optimal value of the parameters for this distribution, and then take 1000 samples from this distribution (e.g., rexp(1000) for an exponential).

Plot a histogram and compare it with a histogram of your non-transformed original variable.   

# Modeling

Build some type of regression model and submit your model to the competition board.  

```{r}
library(caret)
#set up dummy columns
dummies <- dummyVars(SalePrice ~ ., data = train.df)
train.df.dum <- as.data.frame(predict(dummies, newdata = train.df))
train.df.dum <- as.data.frame(train.df.dum %>% dplyr::select(-starts_with(c("Alley"))))
```

##EVAL SET TO FALSE
```{r, eval=FALSE}
library(missForest)
registerDoParallel(cl = makeCluster(25), cores = 100)
set.seed(1234)
train.df.dum.imp <- train.df.dum %>% missForest(maxiter = 10, ntree = 100, replace = TRUE, parallelize = 'forests', verbose = TRUE) #Takes approximately 1 hour to process
write.csv(train.df.dum.imp$ximp,"imputed_training_data.csv", row.names = FALSE) #wrote imputed_data to csv file due to processing time taken by missForest
```

```{r, cache=TRUE, eval=FALSE, results='asis'}
library(caret)
sbf(train.df$LotArea, train.df$SalePrice)

fit <- sbf(form = SalePrice ~ .,
           data = train.df, 
           method = "svmLinear",
           trControl = trainControl(method = "none", 
                                    classProbs = TRUE),
           preProc = c("center", "scale"))

```

```{r, eval=FALSE}
library(leaps)
library(MASS)
regsubsets(SalePrice ~ Id + MSSubClass + MSZoning  + LotArea + Street + LotShape + LandContour + Utilities + LotConfig + LandSlope + Neighborhood + Condition1 + Condition2 + BldgType + HouseStyle + OverallQual + OverallCond + YearBuilt + YearRemodAdd + RoofStyle + RoofMatl + Exterior1st + Exterior2nd + MasVnrType + MasVnrArea + ExterQual + ExterCond + Foundation + BsmtQual + BsmtCond + BsmtExposure + BsmtFinType1 + BsmtFinSF1 + BsmtFinType2 + BsmtFinSF2 + BsmtUnfSF + TotalBsmtSF + Heating + HeatingQC + CentralAir + Electrical + X1stFlrSF + X2ndFlrSF + LowQualFinSF + GrLivArea + BsmtFullBath + BsmtHalfBath + FullBath + HalfBath + BedroomAbvGr + KitchenAbvGr + KitchenQual + TotRmsAbvGrd + Functional + Fireplaces + FireplaceQu + GarageType + GarageYrBlt + GarageFinish + GarageCars + GarageArea + GarageQual + GarageCond + PavedDrive + WoodDeckSF + OpenPorchSF + EnclosedPorch + X3SsnPorch + ScreenPorch + PoolArea + PoolQC + Fence + MiscFeature + MiscVal, data = train.df.dum.imp, nvmax = 10)
```


```{r, eval=FALSE}
library(lme4)
library(nlme)
library(arm)
#testing the random effect
#a first model
mod1<-lme(SalePrice~LotArea+SaleCondition,data=train.df, random=~1|SaleCondition, method="REML")

anova(mod1)
predict(mod1, train.df)
```


Provide your complete model summary and results with analysis. 

Report your Kaggle.com user name and score.

