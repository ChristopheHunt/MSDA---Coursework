---
title: "Final Project"
author: "Christophe Hunt"
date: "May 13, 2017"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    includes:
      in_header: header.tex
    number_sections: yes
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: '2'
always_allow_html: yes
---

```{r load libraries, include=FALSE}
library(tidyverse)
library(scales)
library(forecast)
library(knitr)
library(stargazer)
library(MASS)
library(fitdistrplus)
library(Hmisc)
library(doParallel)
library(DAAG)
library(car)
library(data.table)
library(pander)
```

# Variable

Pick one of the quantitative independent variables from the training data set (train.csv), and define that variable as X.

Pick SalePrice as the dependent variable, and define it as Y for the next analysis.   

## Variable Picked 

> The variable we will set to X is LotArea, which is defined as the Lot size in square feet. I chose LotArea because an anecdotal assumption is that the larger the lot size is the higher the sale price. However, living in NYC, I know that tiny lots in very desirable places have sold for a high price so I believe there may be some interesting varability. 

```{r read train data, message = FALSE, message=FALSE, tidy=TRUE, cache=TRUE}
library(tidyverse)
train.df  <- as_tibble(read.csv(paste("https://raw.githubusercontent.com/", "ChristopheHunt/",
                                      "MSDA---Coursework/master", 
                                      "/Data%20605/Final%20Project/train.csv", sep = "")))
```

```{r subset train data,  message = FALSE, message=FALSE, tidy=TRUE, cache=TRUE}
sub.train.df <- train.df[,c("SalePrice", "LotArea")]
```

# Probability   

Calculate as a minimum the below probabilities a through c.  

Assume the small letter "x" is estimated as the 4th quartile of the X variable, and the small letter "y" is estimated as the 2nd quartile of the Y variable.  Interpret the meaning of all probabilities.  

```{r,message = FALSE, message=FALSE, cache=TRUE}
prob.x <- list(qrt = as.numeric(quantile(sub.train.df$LotArea)[4]))

prob.y <- list(qrt = as.numeric(quantile(sub.train.df$SalePrice)[2]))
```

```{r, cache=TRUE}
prob.y.x <- sub.train.df %>% mutate(greaterLotArea = ifelse(LotArea >= prob.x$qrt, 1, 0), 
                                     lesserLotArea = ifelse(LotArea < prob.x$qrt, 1, 0), 
                                     greaterSalePrice = ifelse(SalePrice >= prob.y$qrt,1, 0), 
                                     lesserSalePrice = ifelse(SalePrice < prob.y$qrt,1, 0))
```

##  a. $P(X>x | Y>y)$	    

```{r, cache=TRUE}
a <- (sum(ifelse(prob.y.x$greaterLotArea == 1 & 
                   prob.y.x$greaterSalePrice == 1, 1, 0)) 
      / nrow(prob.y.x)) / ((sum(prob.y.x$greaterLotArea) / nrow(prob.y.x)))
a
```

##  b. $P(X>x, Y>y)$	
  
```{r, cache=TRUE}
b <- sum(ifelse(prob.y.x$greaterLotArea == 1 & 
                  prob.y.x$greaterSalePrice == 1, 1, 0))/nrow(prob.y.x)
b
```

##  c. $P(X<x | Y>y)$
  
```{r, cache=TRUE}
c <- (sum(ifelse(prob.y.x$lesserLotArea == 1 & 
                   prob.y.x$greaterSalePrice == 1, 1, 0))
      / nrow(prob.y.x)) / ((sum(prob.y.x$lesserLotArea) / nrow(prob.y.x)))
c
```

Does splitting the training data in this fashion make them independent? 

In other words, does $P(X|Y)=P(X)P(Y)$?   

> I am understanding this to mean does the probability of X>x given Y>y, which was answered for in part a. above, equal the probability of X>x mutiplied by Y>y

## Mathematical Check for $P(X|Y)=P(X)P(Y)$

```{r, cache=TRUE}
X <- sum(prob.y.x$greaterLotArea)/ nrow(prob.y.x)
Y <- sum(prob.y.x$greaterSalePrice) / nrow(prob.y.x)
X * Y
```

```{r, cache=TRUE}
a == (X * Y)
```

## Chi Square test for association.  

```{r, cache=TRUE}
prob.table <- as.data.frame(rbind(cbind(sum(prob.y.x$lesserLotArea), sum(prob.y.x$greaterLotArea)), cbind(sum(prob.y.x$lesserSalePrice), sum(prob.y.x$greaterSalePrice))))
chisq.test(prob.table)
```

> We see that the p-value is quite low, lower than the assumptive .05, so we therefore reject the null hypothesis that the values are independent of each other. 

The below venn diagram from Wikipedia may provide a clearer understanding of the differences in these measures:

![](https://raw.githubusercontent.com/ChristopheHunt/MSDA---Coursework/master/Data%20605/Final%20Project/Entropy-mutual-information-relative-entropy-relation-diagram.PNG) [^4]

[^4] By KonradVoelkel (Own work) [Public domain], via Wikimedia Commons   

# Descriptive and Inferential Statistics. 

Provide univariate descriptive statistics and appropriate plots for both variables.

```{r, cache=TRUE, results='asis'}
description <- describe(sub.train.df["LotArea"])
latex(description, file = '')
```

> The histogram in the upper right corner of the table shows a right skewed distribution, which is not surprising since houses in cities would likely have similar relatively smaller lot areas versus instances of large lot areas. 

```{r, cache=TRUE, results='asis'}
description <- describe(sub.train.df["SalePrice"])
latex(description, file = '')
```

> As we can see from the histogram the shape of the data is near normal. It is interesting to visualize that lot area does not follow the same shape, this would hold with our original assumption that where the house is located has more impact than the size of the lot area. 

Provide a scatterplot of X and Y.

```{r scatter plot,  message = FALSE, message=FALSE, tidy=TRUE, cache=TRUE, fig.height=4, fig.width=6}
ggplot(sub.train.df, aes(x = LotArea, y = SalePrice)) +
    geom_point(shape=1) +
    theme_light() +
    scale_y_continuous(labels = dollar)
```

Transform both variables simultaneously using Box-Cox transformations.  

> I am using the `BoxCox.lambda` function from the `forecast` package to determine the necessary transformations for the two variables.

```{r box cox table, cache=TRUE, eval=TRUE}
library(forecast)
library(knitr)
l1 <- BoxCox.lambda(as.numeric(sub.train.df$SalePrice))
l2 <- BoxCox.lambda(as.numeric(sub.train.df$LotArea))

lamdas <- c(l1, l2)
Variables <- c("SalePrice", "LotArea")
dfBoxCox <- as.data.frame(cbind(round(as.numeric(lamdas),4), Variables))
colnames(dfBoxCox) <- c("$\\lambda$", "Variables")
kable(dfBoxCox, align = c("c", "c"))
```

\centering

Common Box-Cox Transformations[^1] [^2]

\setlength{\tabcolsep}{12pt}

\begin{tabular}{ c c }
\hline
$\lambda$ & Y' \\ \hline
-0.5 &	$Y^{-0.5}~=~\frac{1}{\sqrt{(Y)}}$ \\
0	& $\log(Y)$ \\
.25  & $\sqrt[4]{Y}$
\end{tabular}

\justifying

Lambda values were truncated to the nearest tenth that match a common transformation as per the below table.

\centering

\begin{tabular}{ c c }
\hline
variable & variable transformation \\ \hline
SalePrice & $SalePrice^{-0.5}$ \\
LotArea & $log(LotArea)$ 
\end{tabular}

\justifying

\setlength{\tabcolsep}{6pt}

[^1]: Osborne, Jason W. "Improving your data transformations: Applying the Box-Cox transformation." Practical Assessment, Research & Evaluation 15.12 (2010): 1-9.

[^2]: [By Understanding Both the Concept of Transformation and the Box-Cox Method, Practitioners Will Be Better Prepared to Work with Non-normal Data.](https://www.isixsigma.com/tools-templates/normality/making-data-normal-using-box-cox-power-transformation/) . "Making Data Normal Using Box-Cox Power Transformation." ISixSigma. N.p., n.d. Web. 29 Oct. 2016.

## Correlation Analysis

Using the transformed variables, run a correlation analysis and interpret.

```{r, cache=TRUE, eval=TRUE, results='markup'}
sub.train.df.trans <- sub.train.df %>% 
                      mutate(SalePrice = SalePrice^(-.5), 
                             LotArea = log(LotArea))

sub.train.cor <- cor.test(sub.train.df.trans$SalePrice, 
                          sub.train.df.trans$LotArea, 
                          method = "pearson", conf.level = .99)
sub.train.cor
```

> The p-value of the correlation test is 2.2e-16 which is less than the significance level of alpha at .05. We are using the standard alpha as there is no indication another any other value for alpha should be used. We can therefore say that the log of lot size and sale price raised to the -.5 power are significantly correlated with a negative correlation coefficient of -0.386. 

Test the hypothesis that the correlation between these variables is 0 and provide a 99% confidence interval.  

> The correlation test has specifically done that for us and we can safely reject the null hypothesis as we see that our 99% confidence interval exists at the values (-0.441, -0.327) with a p-value < 2.2e-16.

Discuss the meaning of your analysis.

> This means two possible things could have occured, there is no correlation and this data set is pulled from an unusual set of house sales. Or, more likely with the values obtained, our assumption of 0 correlation is incorect and we have obtained a very typical data set and must reject the null hypothesis because correlation does exist. 

# Linear Algebra and Correlation.  

```{r correlation matrix, cache=TRUE, eval=TRUE, results='asis'}
A <- cor(sub.train.df.trans)
kable(A)
```

Invert your correlation matrix.(This is known as the precision matrix and contains variance inflation factors on the diagonal.)

```{r precision matrix, cache=TRUE, eval=TRUE, results='asis'}
B <- solve(A)
kable(B)
```

Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix.
 
```{r matrix multiplication, cache=TRUE, eval=TRUE, results='asis'}
corr.by.pre.M <- A %*% B
kable(corr.by.pre.M)
pre.by.corr.M <- B %*% A
kable(pre.by.corr.M)
```

# Calculus-Based Probability & Statistics

Many times, it makes sense to fit a closed form distribution to data. For your non-transformed independent variable, location shift it so that the minimum value is above zero.

```{r, cache=TRUE, eval=TRUE, results='asis'}
min(sub.train.df$LotArea)
```

> For the independent variable chosen, there are no zero values observed. This makes sense as we would expect the lot area to have some value and I would expect it to never be unobserved (an assumption that at least estimates would be used without a true figure). 

> However, if a shift was required something like the below could be used. 

```{r, cache=TRUE, eval=TRUE, results='asis'}
shift <- sub.train.df$LotArea + 1 
```

Then load the MASS package and run fitdistr to fit a density function of your choice. (See  https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/fitdistr.html).   

> First lets look at what distrubtion would best fit our data.     

```{r, cache=TRUE, eval=TRUE}
library(fitdistrplus)
descdist(sub.train.df$LotArea, discrete=FALSE, boot=500)
```

> There were too many issues in attempting to fit the beta distribution so the next best theoretical distribution was used - log normal. 

```{r, cache=TRUE, eval=TRUE}
library(MASS)
fit.log <- fitdistr(sub.train.df$LotArea, densfun = "log-normal")
fit.log
```
```{r, cache=TRUE, eval=TRUE}
hist(log(sub.train.df$LotArea), prob=TRUE, xlab = "Log of Lot Area", main = "")
curve(dnorm(x, fit.log$estimate[1], fit.log$estimate[2]), col="red", lwd=2, add=T)
```

> From our density plot, the distribution looks quite good. 

Find the optimal value of the parameters for this distribution, and then take 1000 samples from this distribution (e.g., rexp(1000) for an exponential).

```{r, cache=TRUE, eval=TRUE}
set.seed(1234)
sample <- rlnorm(1000, meanlog = fit.log$estimate[1], sdlog = fit.log$estimate[2])
```

Plot a histogram and compare it with a histogram of your non-transformed original variable. 

```{r, cache=TRUE, eval=TRUE}
hist(sample, pch = 20, breaks = 25, col = rgb(1,0,0,0.5), xlim = c(0,50000), ylim = c(0,500), main = 'Overlapping Histogram', xlab = 'Variable') 
hist(sub.train.df$LotArea, pch = 20, breaks = 100, col = rgb(0,0,1,0.5), add = T) 
#https://www.r-bloggers.com/overlapping-histogram-in-r/
legend("topright", c("Sample", "Actual"), col=c(rgb(1,0,0,0.5), rgb(0,0,1,0.5)), lwd=10)
```

 

> It is clear that the distributions are very similar. Plotting them overlapping gives a clear visual of how similar the distributions, note that x has been limited and does not extend out for extreme values of x. 

# Modeling

Build some type of regression model and submit your model to the competition board.  

```{r, results='asis', cache=TRUE}
description <- describe(train.df %>% dplyr::select(-Id, -SalePrice))
latex(description, file = '')
```

> In the variable listing we see many columns with NA values. To simplify our model lets exclude any columns with NAs. 

```{r, eval=TRUE, warning=FALSE, message=FALSE, cache=TRUE}
library(leaps)
train.df.2 <- train.df[ , colSums(is.na(train.df)) == 0]
subsetModel <- regsubsets(SalePrice ~ ., data = train.df.2, 
                          nbest = 1, really.big = T,  method = "seqrep")
summary <- summary(subsetModel, matrix.logical = TRUE)
kable(t(summary$outmat))
```

Based on the last best model we will limit the data set to the following variables MSSubClass, NeighborhoodNoRidge, NeighborhoodNridgHt, NeighborhoodStoneBr, OverallQual, YearRemodAdd, BsmtFinSF1, GrLivArea, GarageCars. 

```{r}
library(DAAG)
fit <- lm(SalePrice ~ MSSubClass + Neighborhood + 
            OverallQual + YearRemodAdd + BsmtFinSF1 + 
            GrLivArea + GarageCars, data = train.df)

p <- par(mfrow=c(2,2))
plot(fit)
par(p)
```

I am slightly concerned that the residual plot does not look like a shotgun pattern. My assumption is that the high influence points and/or outliers may be impacting the model. Therefore, lets use robust linear regression to account for these issues. 

```{r}
BC.MSSubClass <- BoxCox.lambda(as.numeric(train.df$MSSubClass))
BC.Neighborhood <- BoxCox.lambda(as.numeric(train.df$Neighborhood))
BC.OverallQual <- BoxCox.lambda(as.numeric(train.df$OverallQual))
BC.YearRemodAdd <- BoxCox.lambda(as.numeric(train.df$YearRemodAdd))
BC.BsmtFinSF1 <- BoxCox.lambda(as.numeric(train.df$BsmtFinSF1))
BC.GrLivArea <- BoxCox.lambda(as.numeric(train.df$GrLivArea))
BC.GarageCars <- BoxCox.lambda(as.numeric(train.df$GarageCars))
```


```{r}
library(DAAG)
fit <- rlm(SalePrice ~ MSSubClass + Neighborhood + 
            OverallQual + I(YearRemodAdd)^2 + I(BsmtFinSF1) + 
            GrLivArea + GarageCars, SalePrice, data = train.df, k2 = 1.345, wt.method = c("inv.var"), maxit = 20)

#p <- par(mfrow=c(2,2))
plot(fit)
par(p)
```




```{r}
library(car)
library(data.table)
lmfit <- setDT(as.data.frame(car::vif(fit)), keep.rownames = TRUE)[]
lmfit$Adjusted_GVIF <- (lmfit$`GVIF^(1/(2*Df))`^2)
kable(lmfit, align = c("l", "c", "c", "c", "c"))
```

Using GVIF^(1/(2*Df)) [^6] in order to verify that the VIF threshold of 5 for multicollinearity is not exceed. Fortunately, we find that no variable exceeds the threshold and we do not need to adjust for multicollinearity.

[^6]: "Which Variance Inflation Factor Should I Be Using: $GVIF$ or $text{GVIF}^{1/(2cdottext{df})}$?" R. N.p., n.d. Web. 13 Nov. 2016.

## The final model

Provide your complete model summary and results with analysis. 

```{r, results='asis', cache=TRUE}
stargazer(fit, header = FALSE, no.space = TRUE, 
          style = "all2", font.size = "normalsize", 
          single.row = TRUE, intercept.bottom = FALSE)
```

Prediction results with test data set

First there are some observations that are missing values included in our model. For this reason, we will use imputation to complete the cases so our predictions can be carried out. 

```{r, eval=FALSE}
test.df  <- as_tibble(read.csv(paste("https://raw.githubusercontent.com/", "ChristopheHunt/",
                                      "MSDA---Coursework/master", 
                                      "/Data%20605/Final%20Project/test.csv", sep = "")))
library(missForest)
registerDoParallel(cl = makeCluster(8), cores = 8)
set.seed(1234)
test.df.imp <- as.data.frame(test.df %>% dplyr::select(Id, MSSubClass, Neighborhood, OverallQual, YearRemodAdd, BsmtFinSF1, GrLivArea,GarageCars)) %>% missForest(maxiter = 10, ntree = 100, replace = TRUE, parallelize = 'forests', verbose = TRUE) 
write.csv(test.df.imp$ximp,"imputed_test_data.csv", row.names = FALSE) #wrote imputed_data to csv file due to processing time taken by missForest
```


```{r}
test.df  <- as_tibble(read.csv(paste("https://raw.githubusercontent.com/", "ChristopheHunt/",
                                      "MSDA---Coursework/master", 
                                      "/Data%20605/Final%20Project/imputed_test_data.csv", sep = "")))

SalePrice.predict <- predict.lm(fit, type = "response", newdata = test.df)
test.df.wpredict <- cbind(test.df %>% dplyr::select(Id), round(SalePrice.predict,0)) 
colnames(test.df.wpredict)[2] <- c("SalePrice")
pandoc.table(head(test.df.wpredict), split.table = Inf)
```

```{r}
gz1 <- gzfile("submission.csv.gz", "w")
write.csv(test.df.wpredict, gz1, row.names = FALSE)
close(gz1)
```


Kaggle.com user name : 
Kaggle.com score : 

