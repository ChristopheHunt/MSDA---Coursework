---
title: "Assignment 11"
author: "Christophe Hunt"
date: "April 23, 2017"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    includes:
      in_header: header.tex
    number_sections: yes
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: '2'
always_allow_html: yes
---

```{r, include = FALSE}
library(stargazer)
library(tidyverse)
```

\newpage

Using R's lm function, perform regression analysis and measure the significance of the independent variables for the following two data sets.

In the first case, you are evaluating the statement that we hear that Maximum Heart Rate of a person is related to their age by the following equation:

MaxHR = 220 - Age

You have been given the following sample:

Age 18 23 25 35 65 54 34 56 72 19 23 42 18 39 37 
\newline
MaxHR 202 186 187 180 156 169 174 172 153 199 193 174 198 183 178

# Perform a linear regression analysis fitting the Max Heart Rate to Age using the lm function in R. 

## What is the resulting equation?

```{r, results='asis', cache = TRUE}
library(stargazer)
Age <- c(18, 23, 25, 35, 65, 54, 34, 56, 72, 19, 23, 42, 18, 39, 37)
MaxHR <- c(202, 186, 187, 180, 156, 169, 174, 172, 153, 199, 193, 174, 198, 183, 178)
fit <- lm(MaxHR~Age)
stargazer(fit, header = FALSE)
```

## Is the effect of Age on Max HR significant?

> Yes, the effect of Age on MaxH is significant. We see the p<0.01 for Age which indicates that it is indeed signficiant. We would say anything with a p<0.05 is typically significant, depending on the situation.

\newpage

## What is the significance level?

> The signficance level is p<0.01, the exact value is as follows.

```{r , cache = TRUE}
options(scipen = 999)
x <- anova(fit)
x$`Pr(>F)`[1]
```

## Please also plot the fitted relationship between Max HR and Age.

```{r, cache = TRUE}
plot(MaxHR~Age)
```

We can see a very clear linear relationship between the two variables. This helps in confirming that a linear model should be appropriate for this data set.

\newpage

Using the Auto data set from Assignment 5 (also attached here) perform a Linear Regression analysis using mpg as the dependent variable and the other 4 (displacement, horse-power, weight, acceleration) as independent variables.

# What is the final linear regression fit equation Using the Auto data set from Assignment 5?

```{r, cache = TRUE}
library(tidyverse)
df <- as_tibble(read.table(paste0("https://raw.githubusercontent.com",
                                 "/ChristopheHunt/MSDA---Coursework",
                                 "/master/Data%20605/Assignment%2011/",
                                 "auto-mpg.data")))
colnames(df) <- c("displacement", "horsepower", "weight", "acceleration", "mpg")
```

```{r, results='asis', cache = TRUE}
fit <- lm(data = df, formula = (mpg ~ displacement +  horsepower +
                                  weight + acceleration))
stargazer(fit, header = FALSE)
```

\newpage

## Which of the 4 independent variables have a significant impact on mpg?

> Horsepower and weight have a significant impact on mpg. It is unlikely that their values have an impact on mpg merely by chance.

## What are their corresponding significance levels?

```{r, cache = TRUE}
summary(fit)[4]
```

> The variables corresponding significance levels are horsepower = $0.009$ & weight = $0.0000000002$

## What are the standard errors on each of the coefficients?

```{r, cache = TRUE}
summary(fit)[4]$coefficients
```

> The variables corresponding standard errors for our coefficients are horsepower = $0.0166$ & weight = $0.00081$

\newpage

# Please perform this experiment in two ways.

## First take any random 40 data points from the entire auto data sample and perform the linear regression fit and measure the 95% confidence intervals.

```{r, results = 'asis', cache = TRUE}
set.seed(1234)
df_sample <- sample_n(df, 40)
fit.samp <- lm(data = df_sample, formula = (mpg ~ displacement +
                                              horsepower + weight + acceleration))
stargazer(fit.samp, header = FALSE)
```

```{r, cache = TRUE}
names <- colnames(df)[1:4]
for (i in names){
kable(print(confint(fit.samp, i, level = 0.95)))
}
```
> You'll notice that the values that are not significant have confidence intervals that cross 0. If the value could be 0, then its possible that the independent variable impacts our dependent variable merely by chance. 

\newpage

## Then, take the entire data set (all 392 points) and perform linear regression and measure the 95% confidence intervals.

```{r, results = 'asis', cache = TRUE}
fit <- lm(data = df, formula = (mpg ~ displacement +
                                  horsepower + weight + acceleration))
stargazer(fit, header = FALSE)
```

```{r, cache = TRUE}
names <- colnames(df)[1:4]
for (i in names){
kable(print(confint(fit, i, level = 0.95)))
}
```

\newpage

> Let's compare the results of the model using the larger data set and the smaller 40 case sample. 

```{r, cache = TRUE}
names <- colnames(df)[1:4]
for (i in names){
kable(print(confint(fit, i, level = 0.95)))
kable(print(confint(fit.samp, i, level = 0.95)))
}
```

> It is interesting to see how the smaller data set impacts our results. It highlights how important each observation can be in obtaining significant results. It also highlights why we reject the null hypothesis rather than accept the new hypothesis because we can rarely be certain an independent variable is the cause of the change in our dependent variable. A small observation group can give us erroneous results. Hypothesis testing is a powerful tool and one should be aware of the possiblity in making Type I & Type II errors. 